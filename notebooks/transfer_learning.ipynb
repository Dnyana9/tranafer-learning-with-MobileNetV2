{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78b0ec2d-17bd-4ba5-a363-fcc31bff9646",
   "metadata": {},
   "source": [
    "# ü§ü ASL Hand Sign Classifier using Transfer Learning\n",
    "\n",
    "This notebook demonstrates how to build an image classifier for **American Sign Language (ASL)** hand signs using **transfer learning** with MobileNetV2.\n",
    "\n",
    "ASL is a visual language, and recognizing hand gestures accurately can have real-world applications in accessibility tools, education, and human-computer interaction. Instead of training a model from scratch, we leverage a pre-trained CNN (MobileNetV2) and fine-tune it for our 36-class ASL dataset (26 letters + 10 digits).\n",
    "\n",
    "### üß† What You'll Learn:\n",
    "- How to load and preprocess image data efficiently using TensorFlow\n",
    "- How to use data augmentation to improve model generalization\n",
    "- How to apply transfer learning with MobileNetV2\n",
    "- How to fine-tune a model using misclassified validation examples\n",
    "- How to evaluate model performance visually and quantitatively\n",
    "\n",
    "> üì¶ This project uses a preprocessed version of the [Kaggle ASL Alphabet dataset](https://www.kaggle.com/datasets/grassknoted/asl-alphabet), where images are cropped around a pink bounding box to focus on hand signs.\n",
    "\n",
    "Let's get started!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2237a449-d0af-4931-acc2-ec77787887ca",
   "metadata": {},
   "source": [
    "### üì¶ Import Libraries\n",
    "\n",
    "We begin by importing all necessary libraries for data loading, preprocessing, model building, and training:\n",
    "\n",
    "- **Standard libraries** like `os`, `json`, and `pathlib` help manage file paths and directory structures.\n",
    "- **NumPy and Matplotlib** are used for numerical operations and data visualization.\n",
    "- **TensorFlow & Keras** provide tools to build and train deep learning models, including:\n",
    "  - `image_dataset_from_directory` for loading image data\n",
    "  - `MobileNetV2` as our pre-trained feature extractor\n",
    "  - Callbacks like `EarlyStopping` to prevent overfitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a518a215-a603-4cb7-9f58-ac3c08679f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "project_root = Path(\"..\").resolve()  # Or \".\" if notebook is in root\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40a56b8-4ef8-4946-8266-2319f45f7640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports ===\n",
    "import json\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# TensorFlow & Keras\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as tfl\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Rescaling\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "\n",
    "from utils.plotting import plot_training_history_with_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a51acb-f1e4-49bd-aa9f-3c8eb0044c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset path\n",
    "data_dir = Path.cwd().parent / Path(\"data/preprocessed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00da9c15-1973-43d4-b3f5-a2a415625364",
   "metadata": {},
   "source": [
    "### üì• Load & Prepare the Dataset\n",
    "\n",
    "In this section, we define some key hyperparameters and load our preprocessed ASL image dataset.\n",
    "\n",
    "- **Batch size** determines how many images the model sees in one training step.\n",
    "- **Image size** is set to 160√ó160 pixels to match the input shape expected by MobileNetV2.\n",
    "- We use TensorFlow's `image_dataset_from_directory()` to load images from folders and automatically label them based on directory names.\n",
    "\n",
    "We also split the dataset into **80% training** and **20% validation** using the `validation_split` argument, which ensures the model is evaluated on unseen data during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c744ac7c-bae4-488c-bbaa-28bcaf156f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters and image dimensions\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = (160, 160)   # Target image size for model input\n",
    "NUM_CLASSES = 36        # 26 alphabets (A‚ÄìZ) + 10 digits (0‚Äì9)\n",
    "\n",
    "# Load the training dataset from the preprocessed directory\n",
    "train_dataset = image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    shuffle=True,                    # Shuffle for better training\n",
    "    batch_size=BATCH_SIZE,           # Number of images per batch\n",
    "    image_size=IMG_SIZE,             # Resize images to this size\n",
    "    validation_split=0.2,            # Split 20% for validation\n",
    "    subset='training',               # This is the training portion\n",
    "    seed=42                          # Seed for reproducibility\n",
    ")\n",
    "\n",
    "# Load the validation dataset (same split and parameters)\n",
    "validation_dataset = image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    shuffle=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=IMG_SIZE,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',            # validation dataset\n",
    "    seed=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09e9aa5-a260-49ad-8bcd-6d58496c9d43",
   "metadata": {},
   "source": [
    "### üñºÔ∏è Visualize Sample Training Images\n",
    "\n",
    "Before diving into model training, it's a good idea to take a quick look at some sample images from the training dataset. This helps verify that:\n",
    "\n",
    "- Images are loading correctly\n",
    "- Labels match the folder structure\n",
    "- Preprocessing (e.g., resizing) looks as expected\n",
    "\n",
    "Below, we display few images from the training dataset along with their corresponding class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb80f679-ddb0-40bc-a4e5-4f4b38ad155a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_dataset.class_names\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_dataset.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(class_names[labels[i]])\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94f7064-9c0f-41d9-ac6e-01b2ec69fb3b",
   "metadata": {},
   "source": [
    "### üßº Preprocessing & Data Augmentation\n",
    "\n",
    "Before feeding images into our model, we apply two important types of preprocessing:\n",
    "\n",
    "#### 1. MobileNetV2 Input Preprocessing\n",
    "MobileNetV2 expects input pixel values to be scaled to the range **[-1, 1]**.  \n",
    "We apply the `preprocess_input` function provided by TensorFlow to ensure our data is aligned with how the model was originally trained on ImageNet. This improves performance and training stability.\n",
    "\n",
    "#### 2. Data Augmentation\n",
    "To improve generalization and make the model more robust to variations, we apply lightweight data augmentation using the `tf.keras.Sequential` API. This includes:\n",
    "\n",
    "- **Random horizontal flipping** ‚Äì to simulate mirrored hand signs\n",
    "- **Random rotation** ‚Äì to tolerate minor hand tilts\n",
    "\n",
    "These augmentations are applied **only during training**, and help the model learn more flexible feature representations without needing additional data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4696d001-f833-48e9-834e-fbc14f230b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable automatic prefetching for performance\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# Prefetch the next batch while the current one is being processed\n",
    "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# Preprocessing function specific to MobileNetV2\n",
    "# Scales input pixels to the range [-1, 1], matching what the model expects\n",
    "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd7667e-c623-4f0a-90ff-341da7d53119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for data augmentation\n",
    "def data_augmenter():\n",
    "    '''\n",
    "    Create a Sequential model composed of 2 layers\n",
    "    Returns:\n",
    "        tf.keras.Sequential\n",
    "    '''\n",
    "    data_augmentation = tf.keras.Sequential([\n",
    "    tfl.RandomFlip(\"horizontal\"),\n",
    "    tfl.RandomRotation(0.1),\n",
    "])\n",
    "    \n",
    "    return data_augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cfa9c8-7826-4bb6-9196-150d5d048ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = data_augmenter()\n",
    "\n",
    "for image, _ in train_dataset.take(1):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    first_image = image[0]\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        augmented_image = data_augmentation(tf.expand_dims(first_image, 0))\n",
    "        plt.imshow(augmented_image[0] / 255)\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a508d7e-2948-4062-8082-209a41f358ef",
   "metadata": {},
   "source": [
    "### üè∑Ô∏è Inspect Sample Labels\n",
    "\n",
    "Before training, it's useful to inspect a batch of labels to ensure they‚Äôre being loaded and encoded correctly.  \n",
    "In the block below, we print the numeric class labels from the first batch in the training dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34c4461-73b5-4ae9-aa17-0107036a28a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in train_dataset.take(1):\n",
    "    print(labels.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91e79dc-3803-429f-a081-0bb8024f1f36",
   "metadata": {},
   "source": [
    "### üèóÔ∏è Build the Transfer Learning Model\n",
    "\n",
    "We use **MobileNetV2**, a lightweight convolutional neural network pretrained on ImageNet, as our base model. Since it has already learned to extract powerful visual features, we reuse those layers and focus on training a custom classification head for our ASL task.\n",
    "\n",
    "#### Key Steps:\n",
    "- **`include_top=False`** removes the original classification head from MobileNetV2.\n",
    "- **Base model is frozen**, meaning its weights won't be updated during the initial training phase.\n",
    "- **Data augmentation** and **preprocessing** are applied directly to the input layer.\n",
    "- A **Global Average Pooling** layer condenses spatial dimensions, followed by **Dropout** to reduce overfitting.\n",
    "- The final **Dense layer** has 36 neurons with softmax activation ‚Äî one for each ASL class (A‚ÄìZ + 0‚Äì9).\n",
    "\n",
    "This structure allows us to leverage pre-trained knowledge while tailoring the output for our specific classification task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2947ef29-c7d4-4529-bbef-b98553726270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Build the Transfer Learning Model ===\n",
    "def asl_model(image_shape=IMG_SIZE, data_augmentation=data_augmenter()):\n",
    "    ''' Define a tf.keras model for binary classification out of the MobileNetV2 model\n",
    "    Arguments:\n",
    "        image_shape -- Image width and height\n",
    "        data_augmentation -- data augmentation function\n",
    "    Returns:\n",
    "    Returns:\n",
    "        tf.keras.model\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    input_shape = image_shape + (3,)\n",
    "        \n",
    "    base_model = MobileNetV2(input_shape=IMG_SIZE + (3,),\n",
    "                             include_top=False,\n",
    "                             weights='imagenet')\n",
    "    \n",
    "    # freeze the base model by making it non trainable\n",
    "    base_model.trainable = False \n",
    "\n",
    "    # create the input layer (Same as the imageNetv2 input size)\n",
    "    inputs = tf.keras.Input(shape=input_shape) \n",
    "    \n",
    "    # apply data augmentation to the inputs\n",
    "    x = data_augmentation(inputs)\n",
    "    \n",
    "    # data preprocessing using the same weights the model was trained on\n",
    "    x = preprocess_input(x) \n",
    "    \n",
    "    # set training to False to avoid keeping track of statistics in the batch norm layer\n",
    "    x = base_model(x, training=False) \n",
    "    \n",
    "    # add the new Binary classification layers\n",
    "    # use global avg pooling to summarize the info in each channel\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    # include dropout with probability of 0.2 to avoid overfitting\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "        \n",
    "    # use a prediction layer with one neuron (as a binary classifier only needs one)\n",
    "    outputs = tf.keras.layers.Dense(36, activation='softmax')(x)\n",
    "        \n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8f1ad2-601b-41c0-8b0c-5ac23ed4d962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Compile and Train (Frozen Base) ===\n",
    "model = asl_model(IMG_SIZE, data_augmentation)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc5fac9-0563-43fe-aa88-ba9d02513f8f",
   "metadata": {},
   "source": [
    "### üß† Compile & Train the Model\n",
    "\n",
    "Once the model architecture is defined, we compile it with:\n",
    "\n",
    "- **Adam optimizer**: A popular and efficient choice for deep learning models\n",
    "- **Sparse Categorical Crossentropy**: Appropriate for multi-class classification with integer-labeled targets\n",
    "- **Accuracy**: As the evaluation metric\n",
    "\n",
    "We also use **Early Stopping** to monitor validation loss and prevent overfitting. If the model doesn't improve after 5 consecutive epochs, training stops automatically and the best-performing weights are restored.\n",
    "\n",
    "We train the model for up to **30 epochs**, but early stopping may end training earlier if the validation performance plateaus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14312138-58f3-4043-8c3c-ab3f968a3b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop training early if validation loss doesn't improve for 5 epochs\n",
    "early_stop = EarlyStopping(\n",
    "    patience=5,                   # Number of epochs with no improvement before stopping\n",
    "    restore_best_weights=True     # Roll back to the best model weights\n",
    ")\n",
    "\n",
    "# Compile the model with optimizer, loss function, and metric\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),  # Adam optimizer with a low learning rate\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),  # Use for integer-labeled multiclass tasks\n",
    "    metrics=['accuracy']                                     # Track accuracy during training\n",
    ")\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    epochs=30,                   # Max number of epochs\n",
    "    callbacks=[early_stop]       # Stop early if no improvement\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267e033b-e6a6-40c2-9bb4-91fbcfdaaee3",
   "metadata": {},
   "source": [
    "### üìä Visualize Training & Validation Metrics\n",
    "\n",
    "After training the model, it's important to visualize how the training and validation accuracy and loss evolved over epochs. This helps us:\n",
    "\n",
    "- Understand the model's learning progress\n",
    "- Identify overfitting or underfitting\n",
    "- Spot the best performing epoch using early stopping\n",
    "\n",
    "Below, we use a custom helper function `plot_training_history_with_annotations()` to plot:\n",
    "\n",
    "- Training vs. validation **accuracy**\n",
    "- Training vs. validation **loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e0b98d-f190-4805-a636-c29687cd7144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training and validation performance\n",
    "save_path = Path.cwd().parent / \"assets\" / \"training_plot.png\"\n",
    "plot_training_history_with_annotations(history, title=\"Training vs Validation Performance\", save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8610026-847b-402c-895d-6f4f8a36b01d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70ae6441-7cae-4af3-a4da-29177057c7ac",
   "metadata": {},
   "source": [
    "### üîç Identify Misclassified Validation Samples\n",
    "\n",
    "Before fine-tuning the model, we analyze the validation set to identify which examples were misclassified.\n",
    "\n",
    "By doing this, we can:\n",
    "- Detect which classes the model struggles with\n",
    "- Focus training on these ‚Äúhard‚Äù examples\n",
    "- Improve overall accuracy without needing more data\n",
    "\n",
    "In the block below:\n",
    "- We iterate through the validation dataset\n",
    "- Compare predicted labels to ground truth\n",
    "- Store the misclassified images and their corresponding predicted and true labels\n",
    "\n",
    "These hard examples will be used to reweight the training set during fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13025a10-3dd9-40fc-90af-a2276db0cc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify misclassified validation examples for fine-tuning\n",
    "\n",
    "# Lists to store images and predictions where the model got it wrong\n",
    "wrong_images = []\n",
    "wrong_preds = []\n",
    "wrong_labels = []\n",
    "\n",
    "# Class names for mapping predicted labels to text\n",
    "class_names = validation_dataset.class_names\n",
    "\n",
    "# Go through each batch in the validation set\n",
    "for images, labels in validation_dataset:\n",
    "    preds = model.predict(images)                       # Get model predictions\n",
    "    pred_classes = np.argmax(preds, axis=1)             # Convert softmax scores to class indices\n",
    "    true_classes = labels.numpy()                       # Get actual labels\n",
    "\n",
    "    # Compare predictions with ground truth\n",
    "    for i in range(len(images)):\n",
    "        if pred_classes[i] != true_classes[i]:\n",
    "            # Store misclassified image and labels\n",
    "            wrong_images.append(images[i].numpy().astype(\"uint8\"))\n",
    "            wrong_preds.append(pred_classes[i])\n",
    "            wrong_labels.append(true_classes[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41a512f-235f-47da-b99f-4b2794333915",
   "metadata": {},
   "source": [
    "### üîß Prepare Weighted Training Dataset for Fine-Tuning\n",
    "\n",
    "Now that we've identified misclassified samples from the validation set, we create a **weighted training dataset** that emphasizes these hard examples.\n",
    "\n",
    "#### Why?\n",
    "By assigning higher weights to misclassified examples, we:\n",
    "- Encourage the model to pay more attention to these difficult cases\n",
    "- Improve learning on underrepresented or confusing classes\n",
    "- Boost overall validation performance with targeted fine-tuning\n",
    "\n",
    "#### Steps:\n",
    "1. **Unbatch the original training set** so we can add custom weights.\n",
    "2. **Assign a default weight of `1.0`** to all standard training samples.\n",
    "3. **Assign a higher weight of `2.0`** to misclassified examples.\n",
    "4. **Concatenate both datasets** and re-batch them for continued training.\n",
    "\n",
    "This strategy helps the model focus where it previously struggled, without needing to re-train from scratch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e32bbca-635c-49eb-b105-491b3673b057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Add weights to original train_ds (unbatched)\n",
    "train_ds_unbatched = train_dataset.unbatch()\n",
    "\n",
    "def add_default_weight(image, label):\n",
    "    return image, label, tf.constant(1.0, dtype=tf.float32)\n",
    "\n",
    "train_ds_weighted = train_ds_unbatched.map(add_default_weight)\n",
    "\n",
    "# STEP 2: Create a weighted dataset of hard examples\n",
    "wrong_images = np.array(wrong_images)\n",
    "wrong_images = np.array(wrong_images).astype(np.float32)\n",
    "wrong_weights = np.full(len(wrong_labels), 2.0, dtype=np.float32)\n",
    "\n",
    "hard_ds = tf.data.Dataset.from_tensor_slices((wrong_images, wrong_labels, wrong_weights))\n",
    "\n",
    "# STEP 3: Combine and batch\n",
    "combined_train_ds = train_ds_weighted.concatenate(hard_ds)\n",
    "combined_train_ds = (\n",
    "    combined_train_ds.shuffle(1000)\n",
    "                     .cache()\n",
    "                     .repeat()\n",
    "                     .batch(32)\n",
    "                     .prefetch(tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad9a75c-f175-4316-be98-f80f6b949825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With our combined training dataset (standard + hard examples), we now resume training to fine-tune the model.\n",
    "fine_tune_history = model.fit(combined_train_ds, validation_data=validation_dataset, epochs=10, steps_per_epoch=len(train_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3180793a-a657-4701-82ed-2ed507e3f8cd",
   "metadata": {},
   "source": [
    "With our combined training dataset (standard + hard examples), we now resume training to fine-tune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2887228-df73-474b-9397-a907c1101ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's lool at fine tuning results\n",
    "save_path = Path.cwd().parent / \"assets\" / \"fine_tuning_plot.png\"\n",
    "plot_training_history_with_annotations(fine_tune_history, title=\"Fine Tuning Performance\", save_path=save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97b6c80-4e62-4926-8709-4a2f7f838fb0",
   "metadata": {},
   "source": [
    "## üìä Fine-Tuning Results Summary\n",
    "\n",
    "After applying targeted fine-tuning and weighted sampling on hard-to-predict ASL signs, the model achieved significant improvements in performance.\n",
    "\n",
    "### ‚úÖ Key Highlights\n",
    "- üìà **Best Validation Accuracy**: 96% at **Epoch 9**\n",
    "- üîΩ **Lowest Validation Loss**: 0.31 at **Epoch 10**\n",
    "- üîÅ Model was fine-tuned using **MobileNetV2** with **transfer learning**\n",
    "- üß† Boosted generalization by augmenting training data and re-weighting difficult samples\n",
    "- üöÄ Demonstrated that small, strategic adjustments can lead to **large performance gains**\n",
    "\n",
    "\n",
    "üí° _This project proves that targeted model tuning and thoughtful data sampling can make a compact model highly effective‚Äîeven for complex tasks like ASL recognition._\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
